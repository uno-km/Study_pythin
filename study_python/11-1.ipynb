{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     년식   종류    연비   마력    토크   연료  하이브리드   배기량    중량 변속기\n",
      "0  2015  준중형  11.8  172  21.0  가솔린      0  1999  1300  자동\n",
      "1  2015  준중형  12.3  204  27.0  가솔린      0  1591  1300  자동\n",
      "2  2015   소형  15.0  100  13.6  가솔린      0  1368  1035  수동\n",
      "3  2014   소형  14.0  140  17.0  가솔린      0  1591  1090  자동\n",
      "4  2015   대형   9.6  175  46.0   디젤      0  2497  1990  자동\n",
      "Index(['년식', '종류', '연비', '마력', '토크', '연료', '하이브리드', '배기량', '중량', '변속기'], dtype='object')\n",
      "0.7739730315245057\n",
      "[[1.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
      "  1.000e+00 0.000e+00 2.016e+03 6.800e+00 1.590e+02 2.500e+01 0.000e+00\n",
      "  2.359e+03 1.935e+03]]\n",
      "[1802.16030209]\n",
      "1802.1603020886687\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "##########데이터 로드\n",
    "\n",
    "train_df = pd.read_excel('https://github.com/cranberryai/todak_todak_python/blob/master/machine_learning/regression/carprice_E1SUl6b.xlsx?raw=true', sheet_name='train')\n",
    "test_df = pd.read_excel('https://github.com/cranberryai/todak_todak_python/blob/master/machine_learning/regression/carprice_E1SUl6b.xlsx?raw=true', sheet_name='test')\n",
    "\n",
    "##########데이터 분석\n",
    "\n",
    "##########데이터 전처리\n",
    "\n",
    "x_train_df = train_df.drop(['가격'], axis=1)\n",
    "x_test_df = test_df.drop(['가격'], axis=1)\n",
    "y_train_df = train_df['가격']\n",
    "y_test_df = test_df['가격']\n",
    "\n",
    "print(x_train_df.head())\n",
    "'''\n",
    "     년식  종류    연비   마력    토크   연료  하이브리드   배기량    중량 변속기\n",
    "0  2015  대형   6.8  159  23.0  LPG      0  2359  1935  수동\n",
    "1  2012  소형  13.3  108  13.9  가솔린      0  1396  1035  자동\n",
    "2  2015  중형  14.4  184  41.0   디젤      0  1995  1792  자동\n",
    "3  2015  대형  10.9  175  46.0   디젤      0  2497  2210  수동\n",
    "4  2015  대형   6.4  159  23.0  LPG      0  2359  1935  자동\n",
    "'''\n",
    "print(x_train_df.columns) #Index(['년식', '종류', '연비', '마력', '토크', '연료', '하이브리드', '배기량', '중량', '변속기'], dtype='object')\n",
    "\n",
    "transformer = make_column_transformer(\n",
    "    (OneHotEncoder(), ['종류', '연료', '변속기']),\n",
    "    remainder='passthrough')\n",
    "transformer.fit(x_train_df)\n",
    "x_train = transformer.transform(x_train_df)\n",
    "x_test = transformer.transform(x_test_df)\n",
    "\n",
    "y_train = y_train_df.values\n",
    "y_test = y_test_df.values\n",
    "\n",
    "##########모델 학습\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "##########모델 검증\n",
    "\n",
    "print(model.score(x_test, y_test)) #0.7739730315245023\n",
    "\n",
    "##########모델 예측\n",
    "\n",
    "x_test = transformer.transform(pd.DataFrame([\n",
    "    [2016, '대형', 6.8, 159, 25, 'LPG', 0, 2359, 1935, '수동']\n",
    "], columns=['년식', '종류', '연비', '마력', '토크', '연료', '하이브리드', '배기량', '중량', '변속기']))\n",
    "print(x_test)\n",
    "'''\n",
    "[[1.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
    "  1.000e+00 0.000e+00 2.016e+03 6.800e+00 1.590e+02 2.500e+01 0.000e+00\n",
    "  2.359e+03 1.935e+03]]\n",
    "'''\n",
    "\n",
    "y_predict = model.predict(x_test)\n",
    "print(y_predict) #[1802.160302088625]\n",
    "print(y_predict[0]) #1802.160302088625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over support list\n",
      "[[' 98pct. Fat Free Hamburger'], [' Onions'], [' Potato Chips'], [' Hot Dogs'], [' 2pct. Milk'], [' Eggs'], [' White Bread'], [' Cola'], [' Toothpaste'], [' Hamburger Buns'], [' Wheat Bread'], [' Sweet Relish'], [' Toilet Paper']]\n",
      "-----------------\n",
      "over confidence list\n",
      "[' Hamburger Buns'] => [' 98pct. Fat Free Hamburger']: 0.6804123711340206\n",
      "[' Toothpaste'] => [' White Bread']: 0.6018518518518519\n",
      "[' Toothpaste'] => [' Eggs']: 0.5648148148148148\n",
      "[' Wheat Bread'] => [' White Bread']: 0.5619047619047619\n",
      "[' Wheat Bread'] => [' 2pct. Milk']: 0.5523809523809524\n",
      "[' Sweet Relish'] => [' Hot Dogs']: 0.5517241379310344\n",
      "[' Toothpaste'] => [' 2pct. Milk']: 0.5462962962962963\n",
      "[' Onions'] => [' White Bread']: 0.5321100917431193\n",
      "-----------------\n",
      "over lift list\n",
      "[' Hamburger Buns'] => [' 98pct. Fat Free Hamburger']: 7.291663284357497\n",
      "[' 98pct. Fat Free Hamburger'] => [' Hamburger Buns']: 7.291663284357496\n",
      "[' Hot Dogs'] => [' Sweet Relish']: 5.9594964422550625\n",
      "[' Sweet Relish'] => [' Hot Dogs']: 5.959496442255062\n",
      "[' Toothpaste'] => [' Wheat Bread']: 5.640828924162257\n",
      "[' Wheat Bread'] => [' Toothpaste']: 5.640828924162257\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import itertools \n",
    "\n",
    "df = pd.read_csv(\"marketbasket.csv\", index_col=0)\n",
    "\n",
    "def support(df, item_lst):\n",
    "    return (df[list(item_lst)].sum(axis=1)==len(item_lst)).mean()\n",
    "\n",
    "def make_all_set_over_support(df, support_threshold):\n",
    "    items = []\n",
    "    single_items = [col for col in df.columns if support(df, [col]) > support_threshold] # size 1 items\n",
    "    \n",
    "    size = 2\n",
    "    while True:\n",
    "        new_items = []\n",
    "        for item_cand in itertools.combinations(single_items, size):\n",
    "            #print(item_cand, (df[list(item_cand)].sum(axis=1)==size).mean())\n",
    "            if support(df, list(item_cand)) > support_threshold:\n",
    "                new_items.append(list(item_cand))\n",
    "        if len(new_items)==0:\n",
    "            break\n",
    "        else:\n",
    "            items+=new_items\n",
    "            size+=1\n",
    "    items += [ [s] for s in single_items]# 이렇게 해줘야 모든 type이 list가 됨\n",
    "    return items\n",
    "\n",
    "def make_confidence_lst(df, item_set_over_support, confidence_threshold):\n",
    "    r_lst = []\n",
    "    for item1 in item_set_over_support:\n",
    "        for item2 in item_set_over_support:\n",
    "            if len(set(item1).intersection(set(item2)))==0:\n",
    "                conf = support(df, list(set(item1).union(set(item2))))/ support(df, item1)\n",
    "                if conf > confidence_threshold:\n",
    "                    r_lst.append((item1, item2, conf))\n",
    "            else:\n",
    "                continue\n",
    "    return sorted(r_lst, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "def make_lift_lst(df, item_set_over_support, lift_threhsold):\n",
    "    r_lst = []\n",
    "    for item1 in item_set_over_support:\n",
    "        for item2 in item_set_over_support:\n",
    "            if len(set(item1).intersection(set(item2)))==0:\n",
    "                lift = support(df, list(set(item1).union(set(item2))))\n",
    "                lift /= support(df, item1)\n",
    "                lift /= support(df, item2)\n",
    "                if lift > lift_threhsold:\n",
    "                    r_lst.append((item1, item2, lift))\n",
    "            else:\n",
    "                continue\n",
    "    return sorted(r_lst, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "over_support_lst = make_all_set_over_support(df, 0.07)# 0.05로 하면 두 개짜리도 나옴. 로 하면 3개 짜리도 나옴\n",
    "print(\"over support list\")\n",
    "print(over_support_lst)\n",
    "print(\"-----------------\")\n",
    "print(\"over confidence list\")\n",
    "for a, b, conf in  make_confidence_lst(df, over_support_lst, 0.53):\n",
    "    print(\"{} => {}: {}\".format(a, b, conf))\n",
    "print(\"-----------------\")\n",
    "print(\"over lift list\")\n",
    "for a, b, lift in  make_lift_lst(df, over_support_lst, 5.6):\n",
    "    print(\"{} => {}: {}\".format(a, b, lift))\n",
    "import pandas as pd print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9099, 24)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "link_small = pd.read_csv('links_small.csv')\n",
    "link_small = link_small[link_small['tmdbId'].notnull()]['tmdbId'].astype('int') \n",
    "\n",
    "md = pd. read_csv('movies_metadata.csv')\n",
    "\n",
    "md = md.drop([19730, 29503, 35587]) \n",
    "md['id'] = md['id'].astype('int') \n",
    "smd = md[md['id'].isin(link_small)]\n",
    "smd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9099, 268124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "smd['tagline'] = smd['tagline'].fillna('')\n",
    "smd['description'] = smd['overview'] + smd['tagline']\n",
    "smd['description'] = smd['description'].fillna('')\n",
    "tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
    "tfidf_matrix = tf.fit_transform(smd['description'])\n",
    "tfidf_matrix.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7931                      The Dark Knight Rises\n",
       "132                              Batman Forever\n",
       "1113                             Batman Returns\n",
       "8227    Batman: The Dark Knight Returns, Part 2\n",
       "7565                 Batman: Under the Red Hood\n",
       "524                                      Batman\n",
       "7901                           Batman: Year One\n",
       "2579               Batman: Mask of the Phantasm\n",
       "2696                                        JFK\n",
       "8165    Batman: The Dark Knight Returns, Part 1\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "cosine_sim[0]\n",
    "\n",
    "\n",
    "#데이터 정제 중 \n",
    "\n",
    "smd = smd.reset_index()\n",
    "titles = smd['title']\n",
    "indces = pd.Series(smd.index, index=titles)\n",
    "\n",
    "\n",
    "#smd에 인덱스를 포함하고 타이틀을 만든다. pd.Series를 통해서 타이틀을 인덱스로 하고 indces를 만듭니다. \n",
    "\n",
    "def getrecommandations(title):\n",
    "    index = indces[title]\n",
    "    sim_scores = list(enumerate(cosine_sim[index]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x:x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:31]\n",
    "    movie_indices = [i[0] for i in sim_scores] \n",
    "    return titles.iloc[movie_indices]\n",
    "\n",
    "\n",
    "list(enumerate(cosine_sim[0]))\n",
    "getrecommandations('The Dark Knight').head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhd  :  0.4721359549995794\n",
      "kmh  :  0.2402530733520421\n",
      "leb  :  0.2402530733520421\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt \n",
    "\n",
    "critics={\n",
    "     'hhd':{'guardians of the galaxy 2':5,'christmas in august':4,'boss baby':1.5},\n",
    "     'chs':{'christmas in august':5,'boss baby':2},\n",
    "     'kmh':{'guardians of the galaxy 2':2.5,'christmas in august':2,'boss baby':1},\n",
    "     'leb':{'guardians of the galaxy 2':3.5,'christmas in august':4,'boss baby':5}\n",
    "}\n",
    "\n",
    "def sim(i,j):\n",
    "    return sqrt(pow(i,2)+pow(j,2))\n",
    "\n",
    "for i in critics:\n",
    "    if i!='chs':\n",
    "        num1 = critics.get('chs').get('christmas in august')- critics.get(i).get('christmas in august')\n",
    "        num2 = critics.get('chs').get('boss baby')- critics.get(i).get('boss baby')\n",
    "        print(i,\" : \", 1/(1+sim(num1,num2))) #정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.4721359549995794, 'hhd'),\n",
       " (0.2402530733520421, 'leb'),\n",
       " (0.2402530733520421, 'kmh')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sim_distance(data, name1, name2):\n",
    "    sum=0\n",
    "    for i in data[name1]:\n",
    "        if i in data[name2]: #같은 영화를 봤다면\n",
    "            sum+=pow(data[name1][i]- data[name2][i],2)\n",
    "        \n",
    "    return 1/(1+sqrt(sum))\n",
    "\n",
    "\n",
    "def top_match(data, name, index=3, sim_function=sim_distance):\n",
    "    li=[]\n",
    "    for i in data:\n",
    "        if name!=i: #자기 자신은 제외한다\n",
    "            li.append((sim_function(data,name,i),i)) # 유사도, 이름을 튜플에 묶어 리스트에 추가한다\n",
    "    li.sort() #오름차순 정렬\n",
    "    li.reverse() #내림차순 정렬\n",
    "    \n",
    "    return li[:index]\n",
    "\n",
    "top_match(critics, 'chs')\n",
    "# 하나의 함수로 여러개의 아이템을 한 번에 비교했고, \n",
    "# chs과 hhd의 거리가 가장 가까운 것을 확인할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
